# ğŸ“š Deep Learning Experiment Project

Through various deep learning experiments, the theoretical foundations were reproduced in practice and their effects on model performance were analyzed.

## ğŸ“‘ Table of Contents

1. [Sigmoid_vs_ReLU](#ğŸŸ¢-sigmoid_vs_relu)  
2. [BatchNorm_vs_GroupNorm](#ğŸ”µ-batchnorm_vs_groupnorm)  
3. [Conv_Activation_Norm_Position](#ğŸ”¶-conv_activation_norm_position)  
4. [Installation and Execution](#âš™ï¸-installation-and-execution)

---

## ğŸŸ¢ Sigmoid_vs_ReLU

### ğŸ¯ Objective

- Compare activation functions: Sigmoid vs. ReLU  
- Measure performance difference as layer depth (number of blocks) increases fromÂ 1Â toÂ 8

### ğŸ§  Background

The sigmoid function suffers from vanishing gradients in certain regions, making it difficult to train deep networks. ReLU (Rectified Linear Unit), by preserving gradients in the positive region, enables stable training even in deep architectures.

### ğŸ›  Implementation Details

- Notebook: `Sigmoid_vs_ReLU.ipynb`  
- Framework: PyTorch  
- Dataset: CIFAR-10  
- Model design: each block = Convolution â†’ BatchNorm â†’ Activation  
- Branch on activation function (Sigmoid vs. ReLU) only  
- Training settings: identical epochs, learning rate, optimizer  
- Repeated experiments for block countsÂ 1Â throughÂ 8  
- Runtime: Google Colab T4 GPU

### ğŸ“Š Results Summary

| Number of Blocks | Sigmoid Accuracy (%) | ReLU Accuracy (%) |
| ---------------- | -------------------- | ----------------- |
| 1                | 19.00                | 23.01             |
| 2                | 21.75                | 29.19             |
| 3                | 22.00                | 32.63             |
| 4                | 22.95                | 35.18             |
| 5                | 21.93                | 35.18             |
| 6                | 22.55                | 36.20             |
| 7                | 21.18                | 37.49             |
| 8                | 18.59                | 37.61             |

> **Observation**: As depth increases, the Sigmoid modelâ€™s performance deteriorates significantly, whereas the ReLU model maintains stable performance even as it deepens.

---

## ğŸ”µ BatchNorm_vs_GroupNorm

### ğŸ¯ Objective

- Compare normalization methods: BatchNorm vs. GroupNorm  
- Evaluate across batch sizes ofÂ 2,Â 4,Â 8,Â 16,Â 32,Â 64, andÂ 128

### ğŸ§  Background

Batch Normalization is stable when the batch size is large enough, but its statistical estimates become unreliable with small batches, leading to performance drops. Group Normalization uses per-channel group statistics instead of batch statistics, offering consistent performance regardless of batch size.

### ğŸ›  Implementation Details

- Notebooks:  
  - `BatchNorm_vs_GroupNorm.ipynb` (baseline)  
  - `BatchNorm_vs_GroupNorm - Many Epochs.ipynb` (more epochs to match paper settings)  
- Framework: PyTorch  
- Dataset: CIFAR-100  
- Model design: ConvNet blocks (Convolution â†’ Normalization â†’ ReLU)  
  - Branch on normalization layer (BatchNorm vs. GroupNorm)  
  - GroupNorm: fixed toÂ 32Â groups  
- Training settings: identical hyperparameters  
- Repeated experiments for each batch size  

### ğŸ“Š Results Summary

| Batch Size | BatchNorm Accuracy (%) | GroupNorm Accuracy (%) |
| ---------- | ---------------------- | ---------------------- |
| 2          | 10.01                  | 30.89                  |
| 4          | 33.81                  | 40.46                  |
| 8          | 44.55                  | 44.27                  |
| 16         | 49.68                  | 48.85                  |
| 32         | 52.16                  | 50.65                  |
| 64         | 53.18                  | 51.37                  |
| 128        | 52.05                  | 50.09                  |

> **Observation**: BatchNorm lags significantly for batch sizesÂ <Â 8 but matches GroupNorm within 3% for larger batches. GroupNorm remains stable across all sizes.

---

## ğŸ”¶ Conv_Activation_Norm_Position

### ğŸ¯ Objective

Analyze the impact of changing the order of layers (Convolution â†’Â NormalizationÂ â†’Â Activation)

### ğŸ§  Background

The standard sequence is ConvolutionÂ â†’Â BatchNormÂ â†’Â ReLU, but altering this order may affect performance.

### ğŸ›  Implementation Details

- Notebook: `Conv_Activation_Norm_Position.ipynb`  
- Framework: PyTorch  
- Dataset: CIFAR-10  
- Model design: 8 blocks with:  
  1. Conv â†’Â BatchNorm â†’Â ReLU  
  2. Conv â†’Â ReLU â†’Â BatchNorm  
  3. Conv â†’Â LeakyReLU â†’Â BatchNorm  
- Training settings: identical hyperparameters

### ğŸ“Š Results Summary

| Configuration                      | Accuracy (%) |
| ---------------------------------- | ------------ |
| Conv â†’Â BatchNorm â†’Â ReLU            | 62.64        |
| Conv â†’Â ReLU â†’Â BatchNorm            | 58.50        |
| Conv â†’Â LeakyReLU â†’Â BatchNorm       | 60.09        |

> **Analysis**: The standard order (Convâ†’BNâ†’ReLU) yields the highest accuracy, though differences are modest.

---

## âš™ï¸ Installation and Execution

```bash
git clone https://github.com/Lemon-Farm/AI-Experiments.git
cd AI-Experiments
# Open your preferred notebook and run
```
