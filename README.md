# ğŸ“š ë”¥ëŸ¬ë‹ ì‹¤í—˜ í”„ë¡œì íŠ¸

ì„¸ ê°€ì§€ ë”¥ëŸ¬ë‹ ì‹¤í—˜ì„ í†µí•´ í™œì„±í™” í•¨ìˆ˜, ì •ê·œí™” ê¸°ë²•, ë ˆì´ì–´ ë°°ì¹˜ ìˆœì„œê°€ ëª¨ë¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•˜ì˜€ìŠµë‹ˆë‹¤.

## ğŸ“‘ ëª©ì°¨

1. [Sigmoid\_vs\_ReLU](#ğŸŸ¢-sigmoid_vs_relu)
2. [BatchNorm\_vs\_GroupNorm](#ğŸ”µ-batchnorm_vs_groupnorm)
3. [Conv\_Activation\_Norm\_Position](#ğŸ”¶-conv_activation_norm_position)
4. [ì„¤ì¹˜ ë° ì‹¤í–‰ ë°©ë²•](#âš™ï¸-ì„¤ì¹˜-ë°-ì‹¤í–‰-ë°©ë²•)

---

## ğŸŸ¢ Sigmoid\_vs\_ReLU

### ğŸ¯ ëª©í‘œ

* í™œì„±í™” í•¨ìˆ˜ ë¹„êµ: Sigmoid vs ReLU
* ë ˆì´ì–´ ê¹Šì´(ë¸”ë¡ ìˆ˜) 1\~8 ì¦ê°€ ì‹œ ì„±ëŠ¥ ì°¨ì´ í™•ì¸

### ğŸ§  ë°°ê²½

Sigmoid í•¨ìˆ˜ëŠ” íŠ¹ì • êµ¬ê°„ì—ì„œ gradient ì†Œì‹¤(vanishing gradient)ì´ ë°œìƒí•˜ì—¬ ê¹Šì€ ë„¤íŠ¸ì›Œí¬ í•™ìŠµì´ ì–´ë ¤ì›Œì§‘ë‹ˆë‹¤. ReLU(Rectified Linear Unit)ëŠ” ì–‘ìˆ˜ ì˜ì—­ì—ì„œ gradientê°€ ë³´ì¡´ë˜ì–´, ê¹Šì€ êµ¬ì¡°ì—ì„œë„ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ›  êµ¬í˜„ ë‚´ìš©

* ì½”ë“œ: `Sigmoid_vs_ReLU.ipynb`
* í”„ë ˆì„ì›Œí¬: PyTorch
* ë°ì´í„°ì…‹: CIFAR-10
* ëª¨ë¸ êµ¬ì„±: í•˜ë‚˜ì˜ ë¸”ë¡ = Convolution â†’ BatchNorm â†’ Activation
* í™œì„±í™” í•¨ìˆ˜ë§Œ Sigmoid / ReLUë¡œ ë¶„ê¸°
* í•™ìŠµ ì„¤ì •: ë™ì¼í•œ epoch, learning rate, optimizer ì ìš©
* ë°˜ë³µ ì‹¤í—˜: ë¸”ë¡ ìˆ˜ 1\~8 ê°ê°ì— ëŒ€í•´ í•™ìŠµ ë° í‰ê°€
* ì‹¤í—˜ í™˜ê²½: Google Colab T4 GPU

### ğŸ“Š ê²°ê³¼ ìš”ì•½

| ë¸”ë¡ ìˆ˜ | Sigmoid ì •í™•ë„ (%) | ReLU ì •í™•ë„ (%) |
| ---- | --------------- | ------------ |
| 1    | 19.00           | 23.01        |
| 2    | 21.75           | 29.19        |
| 3    | 22.00           | 32.63        |
| 4    | 22.95           | 35.18        |
| 5    | 21.93           | 35.18        |
| 6    | 22.55           | 36.20        |
| 7    | 21.18           | 37.49        |
| 8    | 18.59           | 37.61        |

> **ê´€ì°°**: ë¸”ë¡ ìˆ˜ ì¦ê°€ ì‹œ Sigmoid ëª¨ë¸ì˜ ì„±ëŠ¥ì´ í¬ê²Œ ì €í•˜ë˜ëŠ” ë°˜ë©´, ReLU ëª¨ë¸ì€ ê¹Šì–´ì ¸ë„ ì•ˆì •ì ì¸ ì„±ëŠ¥ ìœ ì§€.

---

## ğŸ”µ BatchNorm\_vs\_GroupNorm

### ğŸ¯ ëª©í‘œ

* ì •ê·œí™” ê¸°ë²• ë¹„êµ: BatchNorm vs GroupNorm
* ë°°ì¹˜ í¬ê¸°(batch size) = 2, 4, 8, 16, 32, 64, 128ë¡œ ì„¤ì •

### ğŸ§  ë°°ê²½

Batch Normalizationì€ ë°°ì¹˜ í¬ê¸°ê°€ ì¶©ë¶„í•  ë•Œ ì•ˆì •ì ì´ë‚˜, ì‘ì„ ê²½ìš° í†µê³„ ì¶”ì •ì´ ë¶ˆì•ˆì •í•˜ì—¬ ì„±ëŠ¥ ì €í•˜. Group Normalizationì€ ë°°ì¹˜ í†µê³„ê°€ ì•„ë‹Œ ì±„ë„ ê·¸ë£¹ í†µê³„ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°°ì¹˜ í¬ê¸°ì— ë¬´ê´€í•œ ì¼ê´€ëœ ì„±ëŠ¥ ì œê³µ.

### ğŸ›  êµ¬í˜„ ë‚´ìš©

* ì½”ë“œ:

  * `BatchNorm_vs_GroupNorm.ipynb` (ê¸°ë³¸ ì‹¤í—˜)
  * `BatchNorm_vs_GroupNorm - Many Epochs.ipynb` (ë…¼ë¬¸ êµ¬í˜„ê³¼ ìœ ì‚¬í•˜ë„ë¡ ì—í¬í¬ ì¦ê°€)
* í”„ë ˆì„ì›Œí¬: PyTorch
* ë°ì´í„°ì…‹: CIFAR-100
* ëª¨ë¸ êµ¬ì„±: ConvNet ë¸”ë¡(Convolution â†’ Norm â†’ ReLU)

  * Norm ë ˆì´ì–´ë§Œ BatchNorm / GroupNorm ë¶„ê¸°
  * GroupNorm: ê·¸ë£¹ ìˆ˜ 32ë¡œ ê³ ì •
* í•™ìŠµ ì¡°ê±´: ë™ì¼í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°
* ë°˜ë³µ ì‹¤í—˜: ê° batch sizeì— ëŒ€í•´ í•™ìŠµ ë° í‰ê°€

### ğŸ“Š ê²°ê³¼ ìš”ì•½

| Batch Size | BatchNorm ì •í™•ë„ (%) | GroupNorm ì •í™•ë„ (%) |
| ---------- | ----------------- | ----------------- |
| 2          | 10.01             | 30.89             |
| 4          | 33.81             | 40.46             |
| 8          | 44.55             | 44.27             |
| 16         | 49.68             | 48.85             |
| 32         | 52.16             | 50.65             |
| 64         | 53.18             | 51.37             |
| 128        | 52.05             | 50.09             |

> **í™•ì¸ì‚¬í•­**: BatchNormì€ batch size â‰¥ 8 ì´ìƒì—ì„œ GroupNorm ëŒ€ë¹„ 3% ì´ë‚´ ì°¨ì´ë¥¼ ë³´ì´ì§€ë§Œ, ê·¸ ì´í•˜ì—ì„œëŠ” ì„±ëŠ¥ì´ ê¸‰ë½. GroupNormì€ ëª¨ë“  ë°°ì¹˜ í¬ê¸°ì—ì„œ ì•ˆì •ì  ì„±ëŠ¥ ìœ ì§€.

---

## ğŸ”¶ Conv\_Activation\_Norm\_Position

### ğŸ¯ ëª©í‘œ

ë ˆì´ì–´ êµ¬ì„± ìˆœì„œ ë³€ê²½(Conv â†’ Norm â†’ Activation)ì˜ ì„±ëŠ¥ ì˜í–¥ ë¶„ì„

### ğŸ§  ë°°ê²½

ì¼ë°˜ì ìœ¼ë¡œ Convolution â†’ BatchNorm â†’ ReLU ìˆœìœ¼ë¡œ êµ¬ì„±í•˜ì§€ë§Œ, ìˆœì„œ ë³€ê²½ì´ ëª¨ë¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì€ ì—°êµ¬ ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤.

### ğŸ›  êµ¬í˜„ ë‚´ìš©

* ì½”ë“œ: `Conv_Activation_Norm_Position.ipynb`
* í”„ë ˆì„ì›Œí¬: PyTorch
* ë°ì´í„°ì…‹: CIFAR-10
* ëª¨ë¸ êµ¬ì„±: ì´ 8ê°œ ë¸”ë¡

  1. Conv â†’ BatchNorm â†’ ReLU
  2. Conv â†’ ReLU â†’ BatchNorm
  3. Conv â†’ LeakyReLU â†’ BatchNorm
* í•™ìŠµ ì¡°ê±´: ë™ì¼í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°

### ğŸ“Š ê²°ê³¼ ìš”ì•½

| êµ¬ì„± ë°©ë²•                        | ì •í™•ë„ (%) |
| ---------------------------- | ------- |
| Conv â†’ BatchNorm â†’ ReLU      | 62.64   |
| Conv â†’ ReLU â†’ BatchNorm      | 58.50   |
| Conv â†’ LeakyReLU â†’ BatchNorm | 60.09   |

> **ë¶„ì„**: ìˆœì„œ ë³€ê²½ì— ë”°ë¥¸ ì •í™•ë„ ì°¨ì´ëŠ” í¬ì§€ ì•Šìœ¼ë‚˜, í‘œì¤€ ìˆœì„œ(Convâ†’BNâ†’ReLU)ê°€ ê°€ì¥ ìš°ìˆ˜í•œ ì„±ëŠ¥ ë³´ì„.

---

## âš™ï¸ ì„¤ì¹˜ ë° ì‹¤í–‰ ë°©ë²•

```bash
git clone https://github.com/yourusername/your-repo.git
cd your-repo
pip install -r requirements.txt
jupyter notebook
```

ê° ì‹¤í—˜ ë…¸íŠ¸ë¶(`.ipynb`)ì„ ì—´ì–´ ì‹¤í–‰í•˜ë©´ ê²°ê³¼ë¥¼ ì¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---
