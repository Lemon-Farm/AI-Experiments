# ğŸ“š ë”¥ëŸ¬ë‹ í™œì„±í™” í•¨ìˆ˜ ë° ì •ê·œí™” ì‹¤í—˜ í”„ë¡œì íŠ¸

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)

ì´ ì €ì¥ì†ŒëŠ” ì„¸ ê°€ì§€ ë”¥ëŸ¬ë‹ ì‹¤í—˜ì„ í†µí•´ í™œì„±í™” í•¨ìˆ˜, ì •ê·œí™” ê¸°ë²•, ë ˆì´ì–´ ë°°ì¹˜ ìˆœì„œê°€ ëª¨ë¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•©ë‹ˆë‹¤.

## ğŸ“‘ ëª©ì°¨

1. [Sigmoid\_vs\_ReLU](#ğŸŸ¢-sigmoid_vs_relu)
2. [BatchNorm\_vs\_GroupNorm](#ğŸ”µ-batchnorm_vs_groupnorm)
3. [Conv\_Activation\_Norm\_Position](#ğŸ”¶-conv_activation_norm_position)
4. [ì„¤ì¹˜ ë° ì‹¤í–‰ ë°©ë²•](#âš™ï¸-ì„¤ì¹˜-ë°-ì‹¤í–‰-ë°©ë²•)
5. [ë¼ì´ì„ ìŠ¤](#ğŸ“-ë¼ì´ì„ ìŠ¤)

---

## ğŸŸ¢ Sigmoid\_vs\_ReLU

### ğŸ¯ ëª©í‘œ

* **í™œì„±í™” í•¨ìˆ˜ ë¹„êµ**: Sigmoid vs ReLU
* **ë ˆì´ì–´ ê¹Šì´ ë³€í™”**: ë¸”ë¡ ìˆ˜ 1\~8 ì¦ê°€ ì‹œ ì„±ëŠ¥ ì°¨ì´ í™•ì¸
* **í‰ê°€ ì ìˆ˜**: 20ì 

### ğŸ§  ë°°ê²½

* Sigmoid í•¨ìˆ˜ëŠ” íŠ¹ì • êµ¬ê°„ì—ì„œ gradient ì†Œì‹¤(vanishing gradient)ì´ ë°œìƒí•˜ì—¬, ê¹Šì€ ë„¤íŠ¸ì›Œí¬ í•™ìŠµì´ ì–´ë ¤ì›Œì§‘ë‹ˆë‹¤.
* ReLU(Rectified Linear Unit)ëŠ” 0 ì´í•˜ ì…ë ¥ì— ëŒ€í•´ gradientê°€ 0ì´ì§€ë§Œ, ì–‘ìˆ˜ì—ì„œëŠ” gradient ë³´ì¡´ì´ ê°€ëŠ¥í•´ ê¹Šì€ êµ¬ì¡°ì—ì„œë„ ì•ˆì •ì ì…ë‹ˆë‹¤.

### ğŸ›  êµ¬í˜„ ë‚´ìš©

* **ì½”ë“œ**: `Sigmoid_vs_ReLU.ipynb`
* **í”„ë ˆì„ì›Œí¬**: PyTorch
* **ë°ì´í„°ì…‹**: CIFAR-10
* **ëª¨ë¸ êµ¬ì„±**:

  * í•˜ë‚˜ì˜ ë¸”ë¡ = Convolution â†’ BatchNorm â†’ Activation
  * í™œì„±í™” í•¨ìˆ˜ë§Œ Sigmoid/ ReLUë¡œ ë¶„ê¸°
* **í•™ìŠµ ì„¤ì •**: Epoch, learning rate, optimizer(ì˜ˆ: Adam) ë“± ëª¨ë“  ì¡°ê±´ ë™ì¼
* **ë°˜ë³µ ì‹¤í—˜**: ë¸”ë¡ ìˆ˜ 1\~8ì— ëŒ€í•´ ê°ê° í•™ìŠµ ë° í‰ê°€

### ğŸ“Š ê²°ê³¼ ìš”ì•½

| ë¸”ë¡ ìˆ˜ | Sigmoid ì •í™•ë„ (%) | ReLU ì •í™•ë„ (%) |
| ---- | --------------- | ------------ |
| 1    | 75.2            | 78.9         |
| 4    | 68.5            | 81.3         |
| 8    | 55.7            | 83.1         |

> **ê´€ì°°**: ë¸”ë¡ ìˆ˜ê°€ ëŠ˜ì–´ë‚ ìˆ˜ë¡ Sigmoid ëª¨ë¸ ì •í™•ë„ê°€ ê¸‰ë½í•˜ëŠ” ë°˜ë©´, ReLU ëª¨ë¸ì€ ì•ˆì •ì ìœ¼ë¡œ ë†’ì€ ì„±ëŠ¥ ìœ ì§€.

---

## ğŸ”µ BatchNorm\_vs\_GroupNorm

### ğŸ¯ ëª©í‘œ

* **ì •ê·œí™” ê¸°ë²• ë¹„êµ**: BatchNorm vs GroupNorm
* **ë°°ì¹˜ í¬ê¸° ë³€í™”**: Batch size = 2,4,8,16,32,64,128

### ğŸ§  ë°°ê²½

* Batch Normalizationì€ ë°°ì¹˜ í¬ê¸°ê°€ ì¶©ë¶„í•  ë•Œ ì•ˆì •ì ì´ë‚˜, ì‘ì„ ê²½ìš° í†µê³„ ì¶”ì •ì´ ë¶ˆì•ˆì •í•˜ì—¬ ì„±ëŠ¥ ì €í•˜.
* Group Normalizationì€ ë°°ì¹˜ í†µê³„ê°€ ì•„ë‹Œ ì±„ë„ ê·¸ë£¹ í†µê³„ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°°ì¹˜ í¬ê¸°ì— ë¬´ê´€í•œ ì¼ê´€ëœ ì„±ëŠ¥ ì œê³µ.

### ğŸ›  êµ¬í˜„ ë‚´ìš©

* **ì½”ë“œ**:

  * `BatchNorm_vs_GroupNorm.ipynb` (ê¸°ë³¸ ì‹¤í—˜)
  * `BatchNorm_vs_GroupNorm - Many Epochs.ipynb` (ë…¼ë¬¸ êµ¬í˜„ê³¼ ìœ ì‚¬í•˜ë„ë¡ ì—í¬í¬ ì¦ê°€)
* **í”„ë ˆì„ì›Œí¬**: PyTorch
* **ë°ì´í„°ì…‹**: CIFAR-100
* **ëª¨ë¸ êµ¬ì„±**: ConvNet ë¸”ë¡(Convolution â†’ Norm â†’ ReLU)

  * Norm ë ˆì´ì–´ë§Œ BatchNorm / GroupNorm ë¶„ê¸°
  * GroupNorm: ê·¸ë£¹ ìˆ˜ ê¸°ë³¸ê°’ 32
* **í•™ìŠµ ì¡°ê±´**: ë™ì¼í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°
* **ë°˜ë³µ ì‹¤í—˜**: ê° batch sizeì— ëŒ€í•´ í•™ìŠµ ë° í‰ê°€

### ğŸ“Š ê²°ê³¼ ìš”ì•½

| Batch Size | BatchNorm ì •í™•ë„ (%) | GroupNorm ì •í™•ë„ (%) |
| ---------- | ----------------- | ----------------- |
| 2          | 42.3              | 64.8              |
| 8          | 70.1              | 69.5              |
| 64         | 75.8              | 76.1              |
| 128        | 76.2              | 76.4              |

> **í™•ì¸ì‚¬í•­**: BatchNormì€ batch size â‰¥ N(â‰ˆ32)ì¼ ë•Œ GroupNorm ëŒ€ë¹„ 3% ì´ë‚´ ì°¨ì´. ê·¸ ì´í•˜ì—ì„œëŠ” ì„±ëŠ¥ í•˜ë½ ëšœë ·. GroupNormì€ ì „ êµ¬ê°„ì—ì„œ ì•ˆì •ì .

---

## ğŸ”¶ Conv\_Activation\_Norm\_Position

### ğŸ¯ ëª©í‘œ

* **ë ˆì´ì–´ ë°°ì¹˜ ìˆœì„œ ë¶„ì„**: Conv â†’ Norm â†’ Activation ìˆœì„œ ë³€ê²½ íš¨ê³¼ ë¹„êµ

### ğŸ§  ë°°ê²½

* ì¼ë°˜ì ìœ¼ë¡œ Convolution â†’ BatchNorm â†’ ReLU ìˆœìœ¼ë¡œ ë ˆì´ì–´ë¥¼ êµ¬ì„±í•˜ì§€ë§Œ, ìˆœì„œ ë³€ê²½ì´ ëª¨ë¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì€ ì—°êµ¬ ì—¬ì§€ê°€ ìˆìŒ.

### ğŸ›  êµ¬í˜„ ë‚´ìš©

* **ì½”ë“œ**: `Conv_Activation_Norm_Position.ipynb`
* **í”„ë ˆì„ì›Œí¬**: PyTorch
* **ë°ì´í„°ì…‹**: CIFAR-10
* **ëª¨ë¸ êµ¬ì„±**: ì´ 8ê°œ ë¸”ë¡

  1. Conv â†’ BatchNorm â†’ ReLU
  2. Conv â†’ ReLU â†’ BatchNorm
  3. Conv â†’ LeakyReLU â†’ BatchNorm
* **í•™ìŠµ ì¡°ê±´**: ë™ì¼í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°

### ğŸ“Š ê²°ê³¼ ìš”ì•½

| êµ¬ì„± ë°©ë²•                        | ì •í™•ë„ (%) |
| ---------------------------- | ------- |
| Conv â†’ BatchNorm â†’ ReLU      | 83.1    |
| Conv â†’ ReLU â†’ BatchNorm      | 82.4    |
| Conv â†’ LeakyReLU â†’ BatchNorm | 82.8    |

> **ë¶„ì„**: ìˆœì„œ ë³€ê²½ì— ë”°ë¥¸ ì •í™•ë„ ì°¨ì´ëŠ” ë¯¸ë¯¸í•˜ë‚˜, í‘œì¤€ ìˆœì„œ(Convâ†’BNâ†’ReLU)ê°€ ìµœìƒ. LeakyReLUëŠ” ReLU ëŒ€ë¹„ ì•½ê°„ ë‚®ì€ ì •í™•ë„.

---

## âš™ï¸ ì„¤ì¹˜ ë° ì‹¤í–‰ ë°©ë²•

```bash
git clone https://github.com/yourusername/your-repo.git
cd your-repo
# í•„ìš” íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt

# Jupyter Notebook ì‹¤í–‰
jupyter notebook
```

ê° ì‹¤í—˜ ë…¸íŠ¸ë¶(`.ipynb`)ì„ ì—´ê³  ì‹¤í–‰í•˜ë©´ ê²°ê³¼ë¥¼ ì¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
